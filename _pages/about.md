---
title: ""
permalink: /
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## About Me

I recently graduated with a M.S. degree in Statistics from [Case Western Reserve University](https://case.edu/). Previously, I obtained my B.Sc. degree in Statistics from [Wuhan University](https://en.whu.edu.cn/). I am currently working on a survey on **causal reasoning with LLMs** under the guidance of [Prof. Jing Ma](https://jma712.github.io/).

## Research Interests

My research interests lie broadly in bridging causal inference and machine learning, exploring their potential applications in AGI. Recently, I'm particularly interested in the intersection of structural causal models (SCMs) and large language models (LLMs). I seek to explore the following topics:

-Does formal causality inspire the explainability of LLMs in entity and event reasoning tasks? How can this contribute to improving LLMs' reliability?
-Why do LLMs struggle with complex causal reasoning tasks? Is it due to their limited mathematical reasoning capabilities through text or the limitation in their capacity to recognize causal graph structure? If we provide text containing information about structural equations, could it help LLMs better identify the feasibility to 'climb the ladder'?
-Is LLM merely a "[causal parrot](https://arxiv.org/pdf/2308.13067)"? Is LLM simply too "lazy" to effectively learn from meta SCMs? Could we potentially identify the origins of its causal knowledge to better evaluate the reliability of the answers?
-Is it possible to extend formal causality to multimodal reasoning tasks?
-...

I am actively seeking Ph.D. / RA positions in the related fields in 2025. Please feel free to contact me at [kangqiliCWRU@gmail.com](kangqiliCWRU@gmail.com) if you are interested in discussing with me.

## Publications & Preprints

\[2024.11\] \[Ongoing\] **Research on formal causal reasoning with large language models.**  
Kangqi Li, Jing Ma

\[2024.10\] \[Under Review\] **Research on benchmark for evaluating the temporal reasoning ability of LVLMs, submitted to a major CV conference.**  
Yiyang Zhou, Linjie Li, Shi Qiu, Zhengyuan Yang, Yuyang Zhao, Yangfan He, **Kangqi Li**, Haonian Ji, Zihao Zhao, Siwei Han, Haibo Tong, Lijuan Wang, Huaxiu Yao
