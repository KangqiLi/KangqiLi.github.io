---
title: ""
permalink: /
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## About Me

I recently graduated with a M.S. degree in Statistics from [Case Western Reserve University](https://case.edu/). Previously, I obtained my B.Sc. degree in Statistics from [Wuhan University](https://en.whu.edu.cn/). I am currently working on a research on **causal reasoning with LLMs** under the guidance of [Prof. Jing Ma](https://jma712.github.io/).

## Research Interests

My research interests lie broadly in bridging causal inference and machine learning, exploring their potential in AI. This includes: **Causality Inspired Trustworthy ML / Graph Out-of-Distribution Generalization / Graph Foundation Models / Causal Reasoning in NLP.** Recently, I'm particularly interested in the **causal reasoning capabilities of LLMs.** I seek to explore the following topics:

- Can formal causality enhance the explainability of LLMs in entity and event reasoning tasks? How might this contribute to improving their reliability?
- Why do LLMs struggle with complex causal reasoning tasks? Is it due to their limited mathematical reasoning capabilities through text, or their inability to learn causal structures? If we provide text-based information about structural equations, could this help LLMs better assess the feasibility of 'climbing the ladder'?
- Is an LLM merely a "[causal parrot](https://arxiv.org/pdf/2308.13067)"? Does fine-tuning LLMs on meta SCM data help mitigate bias and improve generalization?
- What is the ideal benchmark for genuinely evaluating LLMs' causal reasoning capabilities?
- Can formal causality be extended to multimodal reasoning tasks?

I am actively seeking Ph.D. / RA positions in the related fields in 2025. Please feel free to contact me at [kangqiliCWRU@gmail.com](kangqiliCWRU@gmail.com) if you are interested in discussing with me.

## Publications & Preprints

\[2024.11\] \[Ongoing\] **Research on formal causal reasoning with large language models.**  
Kangqi Li, Jing Ma

\[2024.10\] \[Under Review\] **Research on benchmark for evaluating the temporal reasoning ability of LVLMs, submitted to a major CV conference.**  
Yiyang Zhou, Linjie Li, Shi Qiu, Zhengyuan Yang, Yuyang Zhao, Yangfan He, **Kangqi Li**, Haonian Ji, Zihao Zhao, Siwei Han, Haibo Tong, Lijuan Wang, Huaxiu Yao
